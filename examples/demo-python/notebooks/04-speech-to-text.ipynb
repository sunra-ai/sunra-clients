{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Speech-to-Text with Sunra AI\n",
        "\n",
        "This notebook demonstrates how to convert audio files to text using Sunra AI's speech-to-text capabilities.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- How to process audio files\n",
        "- Understanding speech-to-text parameters\n",
        "- Working with different audio formats\n",
        "- Language detection and specification\n",
        "- Speaker diarization (identifying different speakers)\n",
        "- Audio event tagging\n",
        "- Handling long audio files\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup\n",
        "\n",
        "Let's set up our environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sunra_client\n",
        "import getpass\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Set up API key if not already configured\n",
        "if 'SUNRA_KEY' not in os.environ:\n",
        "    api_key = getpass.getpass(\"Enter your Sunra API key: \")\n",
        "    os.environ['SUNRA_KEY'] = api_key\n",
        "\n",
        "# Configure the client\n",
        "sunra_client.config(credentials=os.environ['SUNRA_KEY'])\n",
        "print(\"✓ Sunra client configured successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "Let's create helper functions for speech-to-text processing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcribe_audio(audio_url, language=\"English\", tag_audio_events=True, \n",
        "                     speaker_diarization=False, show_details=True):\n",
        "    \"\"\"\n",
        "    Transcribe audio from a URL\n",
        "    \n",
        "    Args:\n",
        "        audio_url (str): URL of the audio file\n",
        "        language (str): Language of the audio\n",
        "        tag_audio_events (bool): Whether to tag audio events (music, applause, etc.)\n",
        "        speaker_diarization (bool): Whether to identify different speakers\n",
        "        show_details (bool): Whether to show detailed results\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"Transcribing audio from: {audio_url}\")\n",
        "    print(f\"Language: {language}\")\n",
        "    print(f\"Tag audio events: {tag_audio_events}\")\n",
        "    print(f\"Speaker diarization: {speaker_diarization}\")\n",
        "    print(\"Please wait...\")\n",
        "    \n",
        "    try:\n",
        "        result = sunra_client.subscribe(\n",
        "            \"elevenlabs/scribe-v1/speech-to-text\",\n",
        "            arguments={\n",
        "                \"audio\": audio_url,\n",
        "                \"language\": language,\n",
        "                \"tag_audio_events\": tag_audio_events,\n",
        "                \"speaker_diarization\": speaker_diarization\n",
        "            },\n",
        "            with_logs=True,\n",
        "            on_enqueue=lambda req_id: print(f\"✓ Request enqueued: {req_id}\"),\n",
        "            on_queue_update=lambda status: print(f\"Status: {status}\"),\n",
        "        )\n",
        "        \n",
        "        if show_details:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"TRANSCRIPTION RESULTS\")\n",
        "            print(\"=\"*50)\n",
        "            \n",
        "            # Display the main transcript\n",
        "            if result.get('text'):\n",
        "                print(f\"\\nTranscript:\\n{result['text']}\")\n",
        "            \n",
        "            # Display additional details if available\n",
        "            if result.get('segments'):\n",
        "                print(f\"\\nNumber of segments: {len(result['segments'])}\")\n",
        "                \n",
        "                # Show first few segments as examples\n",
        "                print(\"\\nFirst few segments:\")\n",
        "                for i, segment in enumerate(result['segments'][:3]):\n",
        "                    print(f\"  Segment {i+1}:\")\n",
        "                    print(f\"    Text: {segment.get('text', 'N/A')}\")\n",
        "                    print(f\"    Start: {segment.get('start', 'N/A')}s\")\n",
        "                    print(f\"    End: {segment.get('end', 'N/A')}s\")\n",
        "                    if segment.get('speaker'):\n",
        "                        print(f\"    Speaker: {segment['speaker']}\")\n",
        "                    print()\n",
        "            \n",
        "            # Show any detected events\n",
        "            if result.get('events'):\n",
        "                print(f\"Audio events detected: {len(result['events'])}\")\n",
        "                for event in result['events'][:5]:  # Show first 5 events\n",
        "                    print(f\"  - {event}\")\n",
        "            \n",
        "            # Show language detection if available\n",
        "            if result.get('language'):\n",
        "                print(f\"Detected language: {result['language']}\")\n",
        "                \n",
        "            # Show confidence scores if available\n",
        "            if result.get('confidence'):\n",
        "                print(f\"Confidence score: {result['confidence']}\")\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error transcribing audio: {e}\")\n",
        "        return None\n",
        "\n",
        "def format_transcript_with_timestamps(result):\n",
        "    \"\"\"Format transcript with timestamps for better readability\"\"\"\n",
        "    if not result or not result.get('segments'):\n",
        "        return \"No segments available\"\n",
        "    \n",
        "    formatted_transcript = []\n",
        "    \n",
        "    for segment in result['segments']:\n",
        "        start_time = segment.get('start', 0)\n",
        "        end_time = segment.get('end', 0)\n",
        "        text = segment.get('text', '')\n",
        "        speaker = segment.get('speaker', '')\n",
        "        \n",
        "        # Format time as MM:SS\n",
        "        start_formatted = f\"{int(start_time//60):02d}:{int(start_time%60):02d}\"\n",
        "        end_formatted = f\"{int(end_time//60):02d}:{int(end_time%60):02d}\"\n",
        "        \n",
        "        speaker_label = f\" [{speaker}]\" if speaker else \"\"\n",
        "        \n",
        "        formatted_transcript.append(f\"[{start_formatted}-{end_formatted}]{speaker_label}: {text}\")\n",
        "    \n",
        "    return \"\\n\".join(formatted_transcript)\n",
        "\n",
        "print(\"Helper functions ready!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Basic Speech-to-Text\n",
        "\n",
        "Let's start with a simple example using a sample audio file:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic transcription example\n",
        "sample_audio = \"https://assets.sunra.ai/uploads/1749243418768-74d68e25.wav\"\n",
        "\n",
        "print(\"Basic transcription example:\")\n",
        "result = transcribe_audio(sample_audio, language=\"English\", tag_audio_events=True, speaker_diarization=False)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Speaker Diarization\n",
        "\n",
        "Speaker diarization helps identify different speakers in the audio. Let's enable this feature:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transcription with speaker diarization\n",
        "print(\"Transcription with speaker diarization:\")\n",
        "result_with_speakers = transcribe_audio(\n",
        "    sample_audio, \n",
        "    language=\"English\", \n",
        "    tag_audio_events=True, \n",
        "    speaker_diarization=True\n",
        ")\n",
        "\n",
        "# Display formatted transcript with timestamps\n",
        "if result_with_speakers:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FORMATTED TRANSCRIPT WITH TIMESTAMPS\")\n",
        "    print(\"=\"*50)\n",
        "    formatted_transcript = format_transcript_with_timestamps(result_with_speakers)\n",
        "    print(formatted_transcript)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Audio Event Tagging\n",
        "\n",
        "Audio event tagging can identify non-speech sounds like music, applause, laughter, etc. Let's compare with and without this feature:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with and without audio event tagging\n",
        "print(\"=== Without Audio Event Tagging ===\")\n",
        "result_no_events = transcribe_audio(\n",
        "    sample_audio, \n",
        "    language=\"English\", \n",
        "    tag_audio_events=False, \n",
        "    speaker_diarization=False,\n",
        "    show_details=False\n",
        ")\n",
        "\n",
        "print(\"\\n=== With Audio Event Tagging ===\")\n",
        "result_with_events = transcribe_audio(\n",
        "    sample_audio, \n",
        "    language=\"English\", \n",
        "    tag_audio_events=True, \n",
        "    speaker_diarization=False,\n",
        "    show_details=False\n",
        ")\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if result_no_events:\n",
        "    print(\"Without events:\")\n",
        "    print(result_no_events.get('text', 'No text available'))\n",
        "\n",
        "if result_with_events:\n",
        "    print(\"\\nWith events:\")\n",
        "    print(result_with_events.get('text', 'No text available'))\n",
        "    \n",
        "    if result_with_events.get('events'):\n",
        "        print(f\"\\nDetected events: {result_with_events['events']}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Language Support\n",
        "\n",
        "The speech-to-text API supports multiple languages. Let's explore different language options:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common languages supported\n",
        "supported_languages = [\n",
        "    \"English\",\n",
        "    \"Spanish\", \n",
        "    \"French\",\n",
        "    \"German\",\n",
        "    \"Italian\",\n",
        "    \"Portuguese\",\n",
        "    \"Dutch\",\n",
        "    \"Polish\",\n",
        "    \"Russian\",\n",
        "    \"Chinese\",\n",
        "    \"Japanese\",\n",
        "    \"Korean\",\n",
        "    \"Arabic\",\n",
        "    \"Hindi\",\n",
        "    \"Auto\"  # Automatic language detection\n",
        "]\n",
        "\n",
        "print(\"Supported languages:\")\n",
        "for i, lang in enumerate(supported_languages, 1):\n",
        "    print(f\"{i:2d}. {lang}\")\n",
        "\n",
        "# Example with automatic language detection\n",
        "print(\"\\n=== Using Automatic Language Detection ===\")\n",
        "result_auto = transcribe_audio(\n",
        "    sample_audio, \n",
        "    language=\"Auto\", \n",
        "    tag_audio_events=True, \n",
        "    speaker_diarization=False,\n",
        "    show_details=False\n",
        ")\n",
        "\n",
        "if result_auto:\n",
        "    print(f\"Detected language: {result_auto.get('language', 'Unknown')}\")\n",
        "    print(f\"Transcript: {result_auto.get('text', 'No text available')}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Working with Your Own Audio Files\n",
        "\n",
        "To use your own audio files, you'll need to upload them to a publicly accessible URL. Here are the requirements and tips:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_audio_url(audio_url):\n",
        "    \"\"\"Check if an audio URL is accessible\"\"\"\n",
        "    try:\n",
        "        import requests\n",
        "        \n",
        "        print(f\"Checking audio URL: {audio_url}\")\n",
        "        response = requests.head(audio_url)  # Use HEAD to check without downloading\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            print(f\"✓ Audio URL is accessible!\")\n",
        "            \n",
        "            # Check content type if available\n",
        "            content_type = response.headers.get('content-type', 'Unknown')\n",
        "            print(f\"Content type: {content_type}\")\n",
        "            \n",
        "            # Check file size if available\n",
        "            content_length = response.headers.get('content-length')\n",
        "            if content_length:\n",
        "                size_mb = int(content_length) / (1024 * 1024)\n",
        "                print(f\"File size: {size_mb:.2f} MB\")\n",
        "                \n",
        "                if size_mb > 25:  # Many APIs have size limits\n",
        "                    print(\"⚠️  Warning: Large file - may need to be split for some services\")\n",
        "            \n",
        "            return True\n",
        "        else:\n",
        "            print(f\"❌ HTTP Error: {response.status_code}\")\n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error checking URL: {e}\")\n",
        "        return False\n",
        "\n",
        "# Audio format requirements\n",
        "print(\"Audio Format Requirements:\")\n",
        "print(\"- Supported formats: MP3, WAV, FLAC, M4A, OGG\")\n",
        "print(\"- Maximum file size: Usually 25MB (varies by service)\")\n",
        "print(\"- Sample rate: 16kHz or higher recommended\")\n",
        "print(\"- Bit depth: 16-bit or higher\")\n",
        "print(\"- Duration: Up to 30 minutes typically\")\n",
        "\n",
        "print(\"\\nFile Upload Options:\")\n",
        "print(\"1. Cloud storage (Google Drive, Dropbox, etc.)\")\n",
        "print(\"2. Audio hosting services (SoundCloud, etc.)\")\n",
        "print(\"3. Your own web server\")\n",
        "print(\"4. GitHub (for public repositories)\")\n",
        "\n",
        "# Test with sample audio\n",
        "print(\"\\n=== Testing Sample Audio URL ===\")\n",
        "sample_url = \"https://assets.sunra.ai/uploads/1749243418768-74d68e25.wav\"\n",
        "check_audio_url(sample_url)\n",
        "\n",
        "# Uncomment to test your own audio file\n",
        "# print(\"\\n=== Testing Your Audio URL ===\")\n",
        "# your_audio_url = \"YOUR_AUDIO_URL_HERE\"\n",
        "# check_audio_url(your_audio_url)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Advanced Usage Examples\n",
        "\n",
        "Let's create some practical examples for different use cases:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_meeting_transcript(audio_url):\n",
        "    \"\"\"Create a formatted meeting transcript with speakers and timestamps\"\"\"\n",
        "    print(\"Creating meeting transcript...\")\n",
        "    \n",
        "    result = transcribe_audio(\n",
        "        audio_url, \n",
        "        language=\"English\", \n",
        "        tag_audio_events=True, \n",
        "        speaker_diarization=True,\n",
        "        show_details=False\n",
        "    )\n",
        "    \n",
        "    if not result:\n",
        "        return \"Transcription failed\"\n",
        "    \n",
        "    # Create formatted output\n",
        "    output = []\n",
        "    output.append(\"=\" * 60)\n",
        "    output.append(\"MEETING TRANSCRIPT\")\n",
        "    output.append(\"=\" * 60)\n",
        "    output.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    output.append(\"\")\n",
        "    \n",
        "    if result.get('segments'):\n",
        "        current_speaker = None\n",
        "        \n",
        "        for segment in result['segments']:\n",
        "            speaker = segment.get('speaker', 'Unknown')\n",
        "            text = segment.get('text', '').strip()\n",
        "            start_time = segment.get('start', 0)\n",
        "            \n",
        "            # Format time\n",
        "            time_formatted = f\"{int(start_time//60):02d}:{int(start_time%60):02d}\"\n",
        "            \n",
        "            # New speaker or first segment\n",
        "            if speaker != current_speaker:\n",
        "                output.append(f\"\\n[{time_formatted}] {speaker}:\")\n",
        "                current_speaker = speaker\n",
        "            \n",
        "            output.append(f\"  {text}\")\n",
        "    \n",
        "    # Add summary\n",
        "    output.append(\"\\n\" + \"=\" * 60)\n",
        "    output.append(\"SUMMARY\")\n",
        "    output.append(\"=\" * 60)\n",
        "    \n",
        "    if result.get('segments'):\n",
        "        total_duration = max(seg.get('end', 0) for seg in result['segments'])\n",
        "        speakers = list(set(seg.get('speaker', 'Unknown') for seg in result['segments']))\n",
        "        \n",
        "        output.append(f\"Duration: {int(total_duration//60):02d}:{int(total_duration%60):02d}\")\n",
        "        output.append(f\"Speakers: {', '.join(speakers)}\")\n",
        "        output.append(f\"Total segments: {len(result['segments'])}\")\n",
        "    \n",
        "    if result.get('events'):\n",
        "        output.append(f\"Audio events: {', '.join(result['events'])}\")\n",
        "    \n",
        "    return \"\\n\".join(output)\n",
        "\n",
        "def create_interview_transcript(audio_url):\n",
        "    \"\"\"Create a formatted interview transcript\"\"\"\n",
        "    print(\"Creating interview transcript...\")\n",
        "    \n",
        "    result = transcribe_audio(\n",
        "        audio_url, \n",
        "        language=\"English\", \n",
        "        tag_audio_events=False,  # Focus on speech\n",
        "        speaker_diarization=True,\n",
        "        show_details=False\n",
        "    )\n",
        "    \n",
        "    if not result:\n",
        "        return \"Transcription failed\"\n",
        "    \n",
        "    # Create Q&A format\n",
        "    output = []\n",
        "    output.append(\"=\" * 60)\n",
        "    output.append(\"INTERVIEW TRANSCRIPT\")\n",
        "    output.append(\"=\" * 60)\n",
        "    output.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    output.append(\"\")\n",
        "    \n",
        "    if result.get('segments'):\n",
        "        for i, segment in enumerate(result['segments'], 1):\n",
        "            speaker = segment.get('speaker', f'Speaker {i}')\n",
        "            text = segment.get('text', '').strip()\n",
        "            \n",
        "            # Simple Q&A format\n",
        "            label = \"Q:\" if i % 2 == 1 else \"A:\"\n",
        "            output.append(f\"{label} {text}\")\n",
        "            output.append(\"\")\n",
        "    \n",
        "    return \"\\n\".join(output)\n",
        "\n",
        "# Example usage (uncomment to use with your audio)\n",
        "print(\"Advanced transcript formatting examples:\")\n",
        "print(\"1. Meeting transcript with speakers and timestamps\")\n",
        "print(\"2. Interview transcript in Q&A format\")\n",
        "print(\"3. Lecture transcript with events\")\n",
        "\n",
        "# Example with sample audio\n",
        "print(\"\\n=== Sample Meeting Transcript ===\")\n",
        "sample_audio = \"https://assets.sunra.ai/uploads/1749243418768-74d68e25.wav\"\n",
        "meeting_transcript = create_meeting_transcript(sample_audio)\n",
        "print(meeting_transcript[:500] + \"...\" if len(meeting_transcript) > 500 else meeting_transcript)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Audio Quality Matters**: Higher quality audio produces better transcriptions\n",
        "2. **Language Detection**: Use \"Auto\" for mixed-language content or unknown languages\n",
        "3. **Speaker Diarization**: Essential for multi-speaker scenarios like meetings\n",
        "4. **Audio Events**: Helpful for comprehensive transcription of multimedia content\n",
        "5. **URL Requirements**: Audio files must be publicly accessible via direct URLs\n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "- **Audio Preparation**: Use clear, high-quality audio files\n",
        "- **File Formats**: WAV and FLAC provide best quality, MP3 is convenient\n",
        "- **File Size**: Keep files under 25MB for better compatibility\n",
        "- **Language Selection**: Specify language when known for better accuracy\n",
        "- **Use Cases**: Match settings to your specific use case (meeting, interview, lecture)\n",
        "\n",
        "### Common Use Cases:\n",
        "\n",
        "- **Meeting Transcription**: Use speaker diarization and event tagging\n",
        "- **Interview Documentation**: Focus on speaker identification\n",
        "- **Lecture Notes**: Include audio events for comprehensive content\n",
        "- **Podcast Transcription**: Use language detection for varied content\n",
        "- **Voice Memos**: Simple transcription without additional features\n",
        "\n",
        "### Parameters Reference:\n",
        "\n",
        "- `audio`: URL of the audio file (required)\n",
        "- `language`: Language code or \"Auto\" for detection\n",
        "- `tag_audio_events`: Boolean - detect non-speech sounds\n",
        "- `speaker_diarization`: Boolean - identify different speakers\n",
        "\n",
        "### Troubleshooting:\n",
        "\n",
        "1. **Poor Quality**: Check audio quality, reduce background noise\n",
        "2. **URL Errors**: Verify audio file is publicly accessible\n",
        "3. **Language Issues**: Try \"Auto\" detection or specify correct language\n",
        "4. **Long Processing**: Large files take longer, consider splitting\n",
        "5. **Missing Speakers**: Enable speaker diarization for multi-person audio\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Continue to the next notebook: **05-advanced-configuration.ipynb** to learn about advanced SDK configuration options!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
